[8/29/19]
- Sanity check [0.1], 200 samples per class training
	- Baseline: 0.84-0.85
	- Fine-tune [unfreeze all, 20 samples-per]: 0.80
	- Fine-tune [unfreeze bn, output layers, 20]: 0.84
	- Fine-tune [unfreeze output]: 0.84
	- Fine-tune [unfreeze bn, output layers; bn random init.]: 0.80
- Adam optimizer hyperparams: 5e-3 to 5e-5 all reasonable for lr=0.01, 0.001. Even 0 weight decay is not terrible. Tested on baseline.

- Transfer learning [0.1]
	- Baseline: 0.78
	- [unfreeze all]: 0.78
	- [unfreeze bn, output layers, 20]: 0.76
	- [unfreeze output]: 0.72
 	- [unfreeze bn, output layers; bn random init.]: 0.7, w/0.04 std

- Transfer learning [0.2]
	- Baseline: 0.57
	- [unfreeze all]: 0.61
	- [unfreeze bn, output layers]: 0.64 (!?)
	- [unfreeze output]: 0.55

[8/28/19]
- Batchnorm, 0.1: (0.78, 0.015)
  No batchnorm, 0.1: (0.73, 0.03)
  Batchnorm, no dropout: (0.77, 0.018)
- Sanity check: pretrain on same graph and labels [0.1]
	- 0.826 for pretraining w/200 samples-per, 0.851 w/o

[8/27/19]
- 3:1 ring-house graph: 1 iteration of WL, with degree as feature, is sufficient to recover structural labels [TODO: verify]
	- Why are 3 GIN layers needed?

[8/26/19]
Baseline for GNN trained on regular structure graph
- 999_333
	- 3 layers: 100% accuracy for even 1 training label per class
	- Accuracy is less when it's > 3 layers (?!)

- Constant value=1 as feature: accuracy plateaus around 0.5 (?!), for various settings
- 10% noise: using 1 vs. 10 labels makes big difference in performance


[8/23/19]
- GraphWave: edge addition is random over entire graph; labels not updated. Edge deletion is limited to the clique shape; labels of edge nodes update.


[8/6/19]
Publishability
- Dataset too narrow; not accepted by community. Results overfit to it
- Features not good or general enough
	- Pretraining task needs to be somewhat independent of features
	~ Pretraining should work for a variety of structural features -- e.g. LDP, Personalized PR, Kas's 
- Model (MLP) is too small, simple


[8/5/19]
KMeans clustering
- Results same as with no pretraining. Test tr=0.1 only.

[8/1/19]
Degree-based clustering
- Random labels pretrain baseline: similar results to no pretraining
- Varied # clusters as ratio of unique degrees. Clusters defined via degree percentiles
- What cluster ratios improve performance?
	- Brazil: 0.1, 0.2
	- USA: all (?!)
	- Europe: 0.05, 0.1
- For 0.6 train ratio, performance margins are smaller. No improvement on Europe, 1-1.5% improvement on USA

[7/30/19]
LDP feature masking test - 32 combinations
- Parameters: Train ratio = 0.6
- TODO: evaluate results

[7/29/19]
Percentile features - initial impl.
- Some improvement on Brazil dataset (3-4%)
- No improvement or same on Europe, USA
- Normalization is important; improves accuracy significantly

[7/24/19]
Pretraining + lower epoch vs. training w/lower epoch, on same graph
- Idea is to test whether weights transfer minus classifier speeds
	up convergence. This is trivial case, since the pretraining
	is same as training
- Pretrain for 200 epochs
	- epoch=10, USA, tr=0.1: .05-.06 diff
	- epoch=20, USA, tr=0.1: .04-.05 diff
- Too few pretraining epochs hurts perf, too many doesn't 
	- Tested up to 1000 pretrain epochs

Pretraining on different graph: different graphs, same task
- europe -> USA: 100 trials, tr=0.6, epochs-pre = 200, epochs = 200
	- Pretraining: avg 0.609, Std. 0.027
	- Baseline: avg. 0.598, std. 0.027
	- Indistinguishable
- europe -> USA: 100 trials, tr=0.1, epochs-pre = 200, epochs = 200
	- Pretraining: avg 0.589, Std. 0.03
	- Baseline: avg. 0.578, std. 0.03
	- Results within std...
- europe -> USA: 100 trials, tr=0.01, epochs-pre = 200, epochs = 200
	- Pretraining: avg 0.500, Std. 0.059
	- Baseline: avg. 0.498, std. 0.0506
	- Indistinguishable